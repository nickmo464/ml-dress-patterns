{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/usr/lib/python2.7/dist-packages')\n",
    "import numpy as np\n",
    "import cv2\n",
    "import urllib\n",
    "from sklearn import cross_validation\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data is the Crowdflower.com Dress Patterns data set.\n",
    "# https://www.crowdflower.com/wp-content/uploads/2016/07/dress_patterns.csv\n",
    "# load dataset and one-hot encode output classes.\n",
    "imgdf = pd.read_csv('dress_patterns.csv', header=0)\n",
    "categories = pd.get_dummies(imgdf[['category']]).as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import glob\n",
    "#filenames = glob.glob('/home/nick/Documents/LewisUniversity/MachineLearning/Project/images/*.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#fn2=[]\n",
    "#for g in filenames:\n",
    "#    fn2.append(g[-40:])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#imgdf.image_url[3][-40:] in fn2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#for u in imgdf.image_url:\n",
    "#    if u[-40:] in fn2:\n",
    "#        continue\n",
    "#    else:\n",
    "#        #url = urllib.urlopen(u)\n",
    "#        #resource = url.read()\n",
    "#        #outfile = open(\"/home/nick/Documents/LewisUniversity/MachineLearning/Project/\" + u[-40:],\"wb\")\n",
    "#        #outfile.write(resource)\n",
    "#        #outfile.close()\n",
    "#        f = open('/home/nick/Documents/LewisUniversity/MachineLearning/Project/' + u[-40:] + '.txt', 'w')\n",
    "#        f.write('file Not found')\n",
    "#        f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##imgdf.image_url[15701]\n",
    "#filelist = pd.DataFrame(filenames)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print X.shape\n",
    "#print categories.shape\n",
    "#print A.shape\n",
    "\n",
    "#f = open('/home/nick/Documents/LewisUniversity/MachineLearning/Project/status/statuses.txt', 'a')\n",
    "#f.write('hello'+'world'+'\\n')\n",
    "#f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ca5ca27caca94f9fb0617c226477ae35.jpg.png'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dl = imgdf.image_url\n",
    "#for u in fn2:\n",
    "#    if \n",
    "#dl[1][-40:]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#for locs in imgdf.image_url[0:2]:\n",
    "#            fn = \"/home/nick/Documents/LewisUniversity/MachineLearning/Project/images/\" + locs[-40:]\n",
    "#            url = urllib.urlopen(fn)\n",
    "#            resource = url.read()\n",
    "#            imgarr = np.asarray(bytearray(resource), dtype=np.uint8)\n",
    "#            img = cv2.imdecode(imgarr,-1)\n",
    "#            cv2.imshow(\"images\", img)\n",
    "#            cv2.waitKey(0)\n",
    "#            cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "redness = [0,0,255]\n",
    "bound = np.array(redness, dtype = \"uint8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = np.empty(shape=[1]+[3]+[50]+[50], dtype='float32')\n",
    "A = np.empty(shape=[0]+[3]+[50]+[50], dtype='float32')\n",
    "\n",
    "\n",
    "for locs in imgdf.image_url:\n",
    "    \n",
    "       try:\n",
    "            fn = \"/home/nick/Documents/LewisUniversity/MachineLearning/Project/images/\" + locs[-40:]\n",
    "            url = urllib.urlopen(fn)\n",
    "            resource = url.read()\n",
    "            imgarr = np.asarray(bytearray(resource), dtype=np.uint8)\n",
    "            img = cv2.imdecode(imgarr,-1)\n",
    "            \n",
    "            #save the file locally too\n",
    "            #outfile = open(locs[-40:],\"wb\")\n",
    "            #outfile.write(resource)\n",
    "            #outfile.close()\n",
    "\n",
    "            # find the colors within the specified boundaries and apply\n",
    "            mask = cv2.inRange(img, bound, bound)\n",
    "            output = cv2.bitwise_and(img, img, mask = mask)\n",
    "\n",
    "            # find the corners using goodFeaturesToTrack\n",
    "            gray = cv2.cvtColor(output,cv2.COLOR_BGR2GRAY)\n",
    "            corners = cv2.goodFeaturesToTrack(gray,8,0.01,10)\n",
    "            corners = np.int0(corners)\n",
    "\n",
    "            # Get the min and max corners for our rectangle definition.\n",
    "            x1 = None\n",
    "            x2 = None\n",
    "            y1 = None\n",
    "            y2 = None\n",
    "            for i in corners:\n",
    "                a = i[0][0]\n",
    "                b = i[0][1]\n",
    "                if x1 == None:\n",
    "                    x1 = a\n",
    "                    x2 = a\n",
    "                if y1 == None:\n",
    "                    y1 = b\n",
    "                    y2 = b\n",
    "                if x1 != None and a < x1:\n",
    "                    x1 = a\n",
    "                if x2 != None and a > x2:\n",
    "                    x2 = a\n",
    "                if y1 != None and b < y1:\n",
    "                    y1 = b\n",
    "                if y2 != None and b > y2:\n",
    "                    y2 = b\n",
    "\n",
    "            #crop the image to a square based on the middle of rectangle.\n",
    "            # This is so we can have consistent shaped data across all observations.\n",
    "            if (y2-y1) < (x2-x1):\n",
    "                ymin = y1\n",
    "                ymax = y2\n",
    "                xmin = x1+(((x2-x1)-(y2-y1))/2)\n",
    "                xmax = x2-(((x2-x1)-(y2-y1))/2)\n",
    "            else:\n",
    "                ymin = y1+(((y2-y1)-(x2-x1))/2)\n",
    "                ymax = y2-(((y2-y1)-(x2-x1))/2)\n",
    "                xmin = x1\n",
    "                xmax = x2\n",
    "\n",
    "            # show computed image range and display image\n",
    "            #print ymin, ymax, xmin, xmax, ymax-ymin, xmax-xmin    \n",
    "            crop_img = img[ymin:ymax, xmin:xmax] # Crop image\n",
    "\n",
    "            # resize image to scaled 50 by 50\n",
    "            resized_img = cv2.resize(crop_img, (50, 50)) \n",
    "            #hsv_img = cv2.cvtColor(resized_img, cv2.COLOR_RGB2HSV)\n",
    "\n",
    "            # Had messed with using HSV color format, but didn't match with Keras examples.  Back to RGB.\n",
    "            X_temp = resized_img/255.\n",
    "            \n",
    "            # re-arrange array to be a set of 3,50,50 instead of 50,50,3 for the RGB images.\n",
    "            for i in range(3):\n",
    "                for j in range(50):\n",
    "                    for k in range(50):\n",
    "                        X[0,i,j,k] = X_temp[j,k,i]\n",
    "            \n",
    "            \n",
    "            #X = X_temp[:,:,:].flatten()\n",
    "            #Append the record to the array.\n",
    "            \n",
    "            A = np.vstack([A, X])\n",
    "            \n",
    "            #Save a status so we can see how we are doing\n",
    "            f = open('/home/nick/Documents/LewisUniversity/MachineLearning/Project/status/statuses.txt', 'a')\n",
    "            f.write(locs[-40:]+'\\n')\n",
    "            f.close()\n",
    "            \n",
    "\n",
    "       except:\n",
    "            #print \"Bad file. \", fn\n",
    "            f = open('/home/nick/Documents/LewisUniversity/MachineLearning/Project/status/badimages.txt', 'a')\n",
    "            f.write(locs+'\\n')\n",
    "            f.close()\n",
    "\n",
    " \n",
    "\n",
    "            #cv2.imshow(\"images\", img)\n",
    "            #cv2.waitKey(0)\n",
    "            #cv2.destroyAllWindows()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15615, 3, 50, 50)"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Because of a few bad images (grayscale, red rectangle on only 2 sides, etc.), need to remove these from the label set y.\n",
    "\n",
    "badimages = pd.read_csv('/home/nick/Documents/LewisUniversity/MachineLearning/Project/status/badimages.txt', header=None)\n",
    "\n",
    "#categories = pd.get_dummies(imgdf[['category']]).as_matrix()\n",
    "#pd.get_dummies(imgdf[['category']])\n",
    "\n",
    "cleanup = imgdf['image_url'].isin( badimages[0])\n",
    "categories = pd.get_dummies(imgdf.loc[~cleanup].category).as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#imgdf.loc[~cleanup]\n",
    "#imgdf.category [~badimages]\n",
    "#badimages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#imgdf.loc[~cleanup].category.shape\n",
    "#categories[5]\n",
    "#predicted[5]\n",
    "#imgdf[['category']]\n",
    "#categories\n",
    "#pd.get_dummies(imgdf.loc[~cleanup].category)\n",
    "#categories.idxmax(1)\n",
    "#pd.DataFrame(categories).idxmax(1)\n",
    "#categories.shape\n",
    "#y_test.shape\n",
    "#pd.DataFrame(y_test).idxmax(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#save the data set\n",
    "#print locs[-40:]\n",
    "np.save('/home/nick/Documents/LewisUniversity/MachineLearning/Project/visionmatrix',A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15615, 17)\n",
      "(15615, 3, 50, 50)\n"
     ]
    }
   ],
   "source": [
    "#print X.shape\n",
    "print categories.shape\n",
    "print A.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# split up data into train and test sets.\n",
    "X_train, X_test, y_train, y_test = cross_validation.train_test_split(\n",
    "    A, categories, test_size=0.3, random_state=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10930, 3, 50, 50)\n",
      "(10930, 17)\n",
      "(4685, 3, 50, 50)\n",
      "(4685, 17)\n"
     ]
    }
   ],
   "source": [
    "print X_train.shape\n",
    "print y_train.shape\n",
    "print X_test.shape\n",
    "print y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Week 7: Implement Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# re-load the saved data if needed\n",
    "A = np.load('/home/nick/Documents/LewisUniversity/MachineLearning/Project/visionmatrix.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Let's start with the model parameters defined in the Week6 notebook for this data, changing the input shape as appropriate.\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Convolution2D, MaxPooling2D, Flatten\n",
    "from keras.regularizers import l2, l1\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "from keras import backend as K\n",
    "K.set_image_dim_ordering('th')\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Convolution2D(32, 5, 5, border_mode='valid', input_shape=(3, 50, 50), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(17, activation='softmax'))\n",
    "# Compile model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "convolution2d_16 (Convolution2D) (None, 32, 46, 46)    2432        convolution2d_input_11[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_3 (MaxPooling2D)    (None, 32, 23, 23)    0           convolution2d_16[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)              (None, 32, 23, 23)    0           maxpooling2d_3[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "flatten_12 (Flatten)             (None, 16928)         0           dropout_3[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_20 (Dense)                 (None, 128)           2166912     flatten_12[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "dense_21 (Dense)                 (None, 17)            2193        dense_20[0][0]                   \n",
      "====================================================================================================\n",
      "Total params: 2171537\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10930 samples, validate on 4685 samples\n",
      "Epoch 1/10\n",
      "49s - loss: 1.8180 - acc: 0.5196 - val_loss: 1.5847 - val_acc: 0.5895\n",
      "Epoch 2/10\n",
      "48s - loss: 1.4970 - acc: 0.6005 - val_loss: 1.4963 - val_acc: 0.6239\n",
      "Epoch 3/10\n",
      "51s - loss: 1.3796 - acc: 0.6298 - val_loss: 1.4129 - val_acc: 0.6356\n",
      "Epoch 4/10\n",
      "52s - loss: 1.3313 - acc: 0.6319 - val_loss: 1.4162 - val_acc: 0.6211\n",
      "Epoch 5/10\n",
      "54s - loss: 1.2337 - acc: 0.6573 - val_loss: 1.4069 - val_acc: 0.6442\n",
      "Epoch 6/10\n",
      "51s - loss: 1.1610 - acc: 0.6687 - val_loss: 1.5140 - val_acc: 0.6090\n",
      "Epoch 7/10\n",
      "52s - loss: 1.1224 - acc: 0.6770 - val_loss: 1.4472 - val_acc: 0.6333\n",
      "Epoch 8/10\n",
      "51s - loss: 1.0273 - acc: 0.7057 - val_loss: 1.4351 - val_acc: 0.6427\n",
      "Epoch 9/10\n",
      "49s - loss: 0.9726 - acc: 0.7134 - val_loss: 1.4945 - val_acc: 0.6301\n",
      "Epoch 10/10\n",
      "51s - loss: 0.8751 - acc: 0.7409 - val_loss: 1.4887 - val_acc: 0.6312\n"
     ]
    }
   ],
   "source": [
    "#Because of time constraints, we'll just run 10 epochs instead of say, 20.\n",
    "model = model.fit(X_train, y_train,  batch_size = 256,\n",
    "          nb_epoch = 10, verbose=2, validation_data=(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4685/4685 [==============================] - 7s     \n"
     ]
    }
   ],
   "source": [
    "predicted = model.model.predict_classes(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10930, 17)"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9, 4, 9, ..., 9, 9, 9])"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convolution Network Results\n",
      "Confusion Matrix: \n",
      "[[   5    6    2    0   44    0    0    0    0   64    3    0    0    0\n",
      "     0    4    1]\n",
      " [   5   13    3    0   41    0    0    0    0   20    5    0    0    0\n",
      "     0    7    2]\n",
      " [   0    1    2    0   38    0    0    0    0   26    3    0    0    0\n",
      "     0    3    0]\n",
      " [   1    1    0    0   14    0    0    0    0   11    4    0    0    0\n",
      "     0    4    0]\n",
      " [  18   30   13    0  443    1    0    0    0  228   14    0    0    4\n",
      "     0   52   13]\n",
      " [   1    9    0    0   27    0    0    0    0   35    4    0    0    0\n",
      "     0    8    2]\n",
      " [   0    0    0    0    4    0    0    0    0    6    0    0    0    0\n",
      "     0    5    1]\n",
      " [   1    1    1    0   47    1    0    0    0   58    1    0    0    0\n",
      "     0    3    2]\n",
      " [   1    0    0    0    3    0    0    0    2   15    0    0    0    0\n",
      "     0    1    2]\n",
      " [  12    6    4    0   88    0    0    0    0 2385    5    0    0    1\n",
      "     0   13    9]\n",
      " [   4    7    2    0   70    1    0    0    0   72   21    0    0    1\n",
      "     0   16    1]\n",
      " [   0    1    0    0   14    0    0    0    0   18    0    0    0    0\n",
      "     0    0    0]\n",
      " [   0    0    0    0    1    0    0    0    0    1    0    0    0    0\n",
      "     0    0    0]\n",
      " [   2    7    4    0   47    0    0    0    0   58    4    0    0    2\n",
      "     0   16    3]\n",
      " [   1    1    2    0    3    0    0    0    0    8    0    0    0    0\n",
      "     1    3    0]\n",
      " [   2    7    1    0   43    0    0    0    0   67    4    0    0    1\n",
      "     0   79    2]\n",
      " [   5    8    3    0   92    1    0    0    0   47    2    0    0    1\n",
      "     0   11    4]]\n",
      "\n",
      " Classifcation Report\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.09      0.04      0.05       129\n",
      "          1       0.13      0.14      0.13        96\n",
      "          2       0.05      0.03      0.04        73\n",
      "          3       0.00      0.00      0.00        35\n",
      "          4       0.43      0.54      0.48       816\n",
      "          5       0.00      0.00      0.00        86\n",
      "          6       0.00      0.00      0.00        16\n",
      "          7       0.00      0.00      0.00       115\n",
      "          8       1.00      0.08      0.15        24\n",
      "          9       0.76      0.95      0.85      2523\n",
      "         10       0.30      0.11      0.16       195\n",
      "         11       0.00      0.00      0.00        33\n",
      "         12       0.00      0.00      0.00         2\n",
      "         13       0.20      0.01      0.03       143\n",
      "         14       1.00      0.05      0.10        19\n",
      "         15       0.35      0.38      0.37       206\n",
      "         16       0.10      0.02      0.04       174\n",
      "\n",
      "avg / total       0.54      0.63      0.57      4685\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, f1_score, accuracy_score, confusion_matrix\n",
    "\n",
    "# put the y_test back into a format of non-one-hot encoded for comparison\n",
    "y_test_orig = pd.DataFrame(y_test).idxmax(1)\n",
    "print \"Convolution Network Results\\nConfusion Matrix: \"\n",
    "print confusion_matrix(y_test_orig,predicted)\n",
    "print \"\\n Classifcation Report\"\n",
    "print classification_report(y_test_orig,predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model is by no means great, but it does predict with .63 recall and .54 precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's try with the Keras documentation example\n",
    "# apply a 5x5 convolution with 32 output filters on a 50x50 image:\n",
    "model = Sequential()\n",
    "model.add(Convolution2D(16, 3, 3, border_mode='valid', input_shape=(3, 50, 50), activation='relu'))\n",
    "\n",
    "# add a 3x3 convolution on top, with 16 output filters:\n",
    "#model.add(Convolution2D(16, 3, 3, border_mode='same', activation='relu'))\n",
    "\n",
    "# and flatten it and add a dense layer to get it to match the 17 categories.\n",
    "model.add(Flatten())\n",
    "model.add(Dense(17, activation='relu'))\n",
    "# Compile model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "convolution2d_17 (Convolution2D) (None, 16, 48, 48)    448         convolution2d_input_12[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "flatten_13 (Flatten)             (None, 36864)         0           convolution2d_17[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "dense_22 (Dense)                 (None, 17)            626705      flatten_13[0][0]                 \n",
      "====================================================================================================\n",
      "Total params: 627153\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10930 samples, validate on 4685 samples\n",
      "Epoch 1/5\n",
      "15s - loss: 4.6151 - acc: 0.5170 - val_loss: 4.3438 - val_acc: 0.5385\n",
      "Epoch 2/5\n",
      "15s - loss: 4.3493 - acc: 0.5313 - val_loss: 4.2742 - val_acc: 0.5385\n",
      "Epoch 3/5\n",
      "16s - loss: 4.3029 - acc: 0.5313 - val_loss: 4.2773 - val_acc: 0.5385\n",
      "Epoch 4/5\n",
      "16s - loss: 4.2947 - acc: 0.5313 - val_loss: 4.2739 - val_acc: 0.5385\n",
      "Epoch 5/5\n",
      "17s - loss: 4.2916 - acc: 0.5313 - val_loss: 4.2709 - val_acc: 0.5385\n"
     ]
    }
   ],
   "source": [
    "model = model.fit(X_train, y_train,  batch_size = 256,\n",
    "          nb_epoch = 5, verbose=2, validation_data=(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4672/4685 [============================>.] - ETA: 0s"
     ]
    }
   ],
   "source": [
    "predicted = model.model.predict_classes(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convolution Network Results\n",
      "Confusion Matrix: \n",
      "[[   0    0    0    0    0    0    0    0    0  129    0    0    0    0\n",
      "     0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0   96    0    0    0    0\n",
      "     0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0   73    0    0    0    0\n",
      "     0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0   35    0    0    0    0\n",
      "     0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0  816    0    0    0    0\n",
      "     0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0   86    0    0    0    0\n",
      "     0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0   16    0    0    0    0\n",
      "     0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0  115    0    0    0    0\n",
      "     0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0   24    0    0    0    0\n",
      "     0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0 2523    0    0    0    0\n",
      "     0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0  195    0    0    0    0\n",
      "     0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0   33    0    0    0    0\n",
      "     0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    2    0    0    0    0\n",
      "     0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0  143    0    0    0    0\n",
      "     0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0   19    0    0    0    0\n",
      "     0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0  206    0    0    0    0\n",
      "     0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0  174    0    0    0    0\n",
      "     0    0    0]]\n",
      "\n",
      " Classifcation Report\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.00      0.00      0.00       129\n",
      "          1       0.00      0.00      0.00        96\n",
      "          2       0.00      0.00      0.00        73\n",
      "          3       0.00      0.00      0.00        35\n",
      "          4       0.00      0.00      0.00       816\n",
      "          5       0.00      0.00      0.00        86\n",
      "          6       0.00      0.00      0.00        16\n",
      "          7       0.00      0.00      0.00       115\n",
      "          8       0.00      0.00      0.00        24\n",
      "          9       0.54      1.00      0.70      2523\n",
      "         10       0.00      0.00      0.00       195\n",
      "         11       0.00      0.00      0.00        33\n",
      "         12       0.00      0.00      0.00         2\n",
      "         13       0.00      0.00      0.00       143\n",
      "         14       0.00      0.00      0.00        19\n",
      "         15       0.00      0.00      0.00       206\n",
      "         16       0.00      0.00      0.00       174\n",
      "\n",
      "avg / total       0.29      0.54      0.38      4685\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# put the y_test back into a format of non-one-hot encoded for comparison\n",
    "y_test_orig = pd.DataFrame(y_test).idxmax(1)\n",
    "print \"Convolution Network Results\\nConfusion Matrix: \"\n",
    "print confusion_matrix(y_test_orig,predicted)\n",
    "print \"\\n Classifcation Report\"\n",
    "print classification_report(y_test_orig,predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a much worse model, and it is always predicting a value of 7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's try something totally different, a neural network based on the homework from week 5: \n",
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=(3, 50, 50)))\n",
    "model.add(Dense(output_dim=100,\n",
    "                activation='sigmoid', W_regularizer=l2(0.01)))\n",
    "model.add(Dense(output_dim=500, activation='sigmoid', W_regularizer=l2(0.01)))\n",
    "model.add(Dense(output_dim=17, activation='sigmoid', W_regularizer=l2(0.01)))\n",
    "\n",
    "\n",
    "# Compile model\n",
    "sgd = SGD(lr=0.1)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "flatten_15 (Flatten)             (None, 7500)          0           flatten_input_4[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "dense_26 (Dense)                 (None, 100)           750100      flatten_15[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "dense_27 (Dense)                 (None, 500)           50500       dense_26[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_28 (Dense)                 (None, 17)            8517        dense_27[0][0]                   \n",
      "====================================================================================================\n",
      "Total params: 809117\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10930 samples, validate on 4685 samples\n",
      "Epoch 1/20\n",
      "1s - loss: 5.5208 - acc: 0.5178 - val_loss: 1.7018 - val_acc: 0.5385\n",
      "Epoch 2/20\n",
      "1s - loss: 4.8105 - acc: 0.5313 - val_loss: 1.6959 - val_acc: 0.5385\n",
      "Epoch 3/20\n",
      "1s - loss: 4.3234 - acc: 0.5313 - val_loss: 1.6966 - val_acc: 0.5385\n",
      "Epoch 4/20\n",
      "1s - loss: 3.9117 - acc: 0.5313 - val_loss: 1.6977 - val_acc: 0.5385\n",
      "Epoch 5/20\n",
      "1s - loss: 3.5659 - acc: 0.5313 - val_loss: 1.6968 - val_acc: 0.5385\n",
      "Epoch 6/20\n",
      "1s - loss: 3.2751 - acc: 0.5313 - val_loss: 1.6926 - val_acc: 0.5385\n",
      "Epoch 7/20\n",
      "1s - loss: 3.0324 - acc: 0.5313 - val_loss: 1.6932 - val_acc: 0.5385\n",
      "Epoch 8/20\n",
      "1s - loss: 2.8245 - acc: 0.5313 - val_loss: 1.6977 - val_acc: 0.5385\n",
      "Epoch 9/20\n",
      "1s - loss: 2.6517 - acc: 0.5313 - val_loss: 1.6941 - val_acc: 0.5385\n",
      "Epoch 10/20\n",
      "1s - loss: 2.5054 - acc: 0.5313 - val_loss: 1.6951 - val_acc: 0.5385\n",
      "Epoch 11/20\n",
      "1s - loss: 2.3836 - acc: 0.5313 - val_loss: 1.6920 - val_acc: 0.5385\n",
      "Epoch 12/20\n",
      "1s - loss: 2.2798 - acc: 0.5313 - val_loss: 1.6921 - val_acc: 0.5385\n",
      "Epoch 13/20\n",
      "1s - loss: 2.1941 - acc: 0.5313 - val_loss: 1.7108 - val_acc: 0.5385\n",
      "Epoch 14/20\n",
      "1s - loss: 2.1216 - acc: 0.5313 - val_loss: 1.6953 - val_acc: 0.5385\n",
      "Epoch 15/20\n",
      "1s - loss: 2.0573 - acc: 0.5313 - val_loss: 1.6970 - val_acc: 0.5385\n",
      "Epoch 16/20\n",
      "1s - loss: 2.0051 - acc: 0.5313 - val_loss: 1.7081 - val_acc: 0.5385\n",
      "Epoch 17/20\n",
      "1s - loss: 1.9616 - acc: 0.5313 - val_loss: 1.6993 - val_acc: 0.5385\n",
      "Epoch 18/20\n",
      "1s - loss: 1.9246 - acc: 0.5313 - val_loss: 1.6971 - val_acc: 0.5385\n",
      "Epoch 19/20\n",
      "1s - loss: 1.8942 - acc: 0.5313 - val_loss: 1.7015 - val_acc: 0.5385\n",
      "Epoch 20/20\n",
      "1s - loss: 1.8683 - acc: 0.5313 - val_loss: 1.6971 - val_acc: 0.5385\n"
     ]
    }
   ],
   "source": [
    "# Fit the model\n",
    "model = model.fit(X_train, y_train, batch_size = 256,\n",
    "          nb_epoch = 20, verbose=2, validation_data=(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4685/4685 [==============================] - 0s     \n"
     ]
    }
   ],
   "source": [
    "predicted = model.model.predict_classes(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network Results\n",
      "Confusion Matrix: \n",
      "[[   0    0    0    0    0    0    0    0    0  129    0    0    0    0\n",
      "     0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0   96    0    0    0    0\n",
      "     0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0   73    0    0    0    0\n",
      "     0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0   35    0    0    0    0\n",
      "     0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0  816    0    0    0    0\n",
      "     0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0   86    0    0    0    0\n",
      "     0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0   16    0    0    0    0\n",
      "     0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0  115    0    0    0    0\n",
      "     0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0   24    0    0    0    0\n",
      "     0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0 2523    0    0    0    0\n",
      "     0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0  195    0    0    0    0\n",
      "     0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0   33    0    0    0    0\n",
      "     0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    2    0    0    0    0\n",
      "     0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0  143    0    0    0    0\n",
      "     0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0   19    0    0    0    0\n",
      "     0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0  206    0    0    0    0\n",
      "     0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0  174    0    0    0    0\n",
      "     0    0    0]]\n",
      "\n",
      " Classifcation Report\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.00      0.00      0.00       129\n",
      "          1       0.00      0.00      0.00        96\n",
      "          2       0.00      0.00      0.00        73\n",
      "          3       0.00      0.00      0.00        35\n",
      "          4       0.00      0.00      0.00       816\n",
      "          5       0.00      0.00      0.00        86\n",
      "          6       0.00      0.00      0.00        16\n",
      "          7       0.00      0.00      0.00       115\n",
      "          8       0.00      0.00      0.00        24\n",
      "          9       0.54      1.00      0.70      2523\n",
      "         10       0.00      0.00      0.00       195\n",
      "         11       0.00      0.00      0.00        33\n",
      "         12       0.00      0.00      0.00         2\n",
      "         13       0.00      0.00      0.00       143\n",
      "         14       0.00      0.00      0.00        19\n",
      "         15       0.00      0.00      0.00       206\n",
      "         16       0.00      0.00      0.00       174\n",
      "\n",
      "avg / total       0.29      0.54      0.38      4685\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# put the y_test back into a format of non-one-hot encoded for comparison\n",
    "y_test_orig = pd.DataFrame(y_test).idxmax(1)\n",
    "print \"Neural Network Results\\nConfusion Matrix: \"\n",
    "print confusion_matrix(y_test_orig,predicted)\n",
    "print \"\\n Classifcation Report\"\n",
    "print classification_report(y_test_orig,predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, this is a lousy model like the previous one, as it just predicts 9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# let's try logistic regression.\n",
    "# Stochastic Logistic Regression\n",
    "model = Sequential()\n",
    "\n",
    "# validation loss \n",
    "model.add(Flatten(input_shape=(3, 50, 50)))\n",
    "model.add(Dense(output_dim=17, activation='sigmoid', W_regularizer=l1(0.01)))\n",
    "\n",
    "# Compile model\n",
    "sgd = SGD(lr=0.1)\n",
    "model.compile(loss='mean_squared_error', optimizer=sgd, metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "flatten_16 (Flatten)             (None, 7500)          0           flatten_input_5[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "dense_29 (Dense)                 (None, 17)            127517      flatten_16[0][0]                 \n",
      "====================================================================================================\n",
      "Total params: 127517\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "model = model.fit(X_train, y_train, batch_size = 256,\n",
    "          nb_epoch = 100, verbose=0, validation_data=(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4576/4685 [============================>.] - ETA: 0s"
     ]
    }
   ],
   "source": [
    "predicted = model.model.predict_classes(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Results\n",
      "Confusion Matrix: \n",
      "[[   0    0    0    0    0    0    0    0    0  129    0    0    0    0\n",
      "     0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0   96    0    0    0    0\n",
      "     0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0   73    0    0    0    0\n",
      "     0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0   35    0    0    0    0\n",
      "     0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0  816    0    0    0    0\n",
      "     0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0   86    0    0    0    0\n",
      "     0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0   16    0    0    0    0\n",
      "     0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0  115    0    0    0    0\n",
      "     0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0   24    0    0    0    0\n",
      "     0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0 2523    0    0    0    0\n",
      "     0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0  195    0    0    0    0\n",
      "     0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0   33    0    0    0    0\n",
      "     0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    2    0    0    0    0\n",
      "     0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0  143    0    0    0    0\n",
      "     0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0   19    0    0    0    0\n",
      "     0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0  206    0    0    0    0\n",
      "     0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0  174    0    0    0    0\n",
      "     0    0    0]]\n",
      "\n",
      " Classifcation Report\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.00      0.00      0.00       129\n",
      "          1       0.00      0.00      0.00        96\n",
      "          2       0.00      0.00      0.00        73\n",
      "          3       0.00      0.00      0.00        35\n",
      "          4       0.00      0.00      0.00       816\n",
      "          5       0.00      0.00      0.00        86\n",
      "          6       0.00      0.00      0.00        16\n",
      "          7       0.00      0.00      0.00       115\n",
      "          8       0.00      0.00      0.00        24\n",
      "          9       0.54      1.00      0.70      2523\n",
      "         10       0.00      0.00      0.00       195\n",
      "         11       0.00      0.00      0.00        33\n",
      "         12       0.00      0.00      0.00         2\n",
      "         13       0.00      0.00      0.00       143\n",
      "         14       0.00      0.00      0.00        19\n",
      "         15       0.00      0.00      0.00       206\n",
      "         16       0.00      0.00      0.00       174\n",
      "\n",
      "avg / total       0.29      0.54      0.38      4685\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# put the y_test back into a format of non-one-hot encoded for comparison\n",
    "y_test_orig = pd.DataFrame(y_test).idxmax(1)\n",
    "print \"Logistic Regression Results\\nConfusion Matrix: \"\n",
    "print confusion_matrix(y_test_orig,predicted)\n",
    "print \"\\n Classifcation Report\"\n",
    "print classification_report(y_test_orig,predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These neural network based models are giving the same results, except for the convolutional network.  We'll try it again for a logistic regression, but pre-flattening the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# reshape the training and test data\n",
    "X_train_new = np.empty(shape=[X_train.shape[0]] + [7500], dtype='float32')\n",
    "for i in range(X_train.shape[0]):\n",
    "    X_train_new[i,:] = X_train[i,:,:,:].flatten()\n",
    "\n",
    "X_test_new = np.empty(shape=[X_test.shape[0]] + [7500], dtype='float32')\n",
    "for i in range(X_test.shape[0]):\n",
    "    X_test_new[i,:] = X_test[i,:,:,:].flatten()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4685, 7500)\n",
      "(10930, 7500)\n"
     ]
    }
   ],
   "source": [
    "print X_test_new.shape\n",
    "print X_train_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Stochastic Logistic Regression\n",
    "model = Sequential()\n",
    "\n",
    "# validation loss \n",
    "model.add(Dense(output_dim=17, input_shape=[7500], \n",
    "                activation='sigmoid', W_regularizer=l2(0)))\n",
    "\n",
    "# Compile model\n",
    "sgd = SGD(lr=0.1)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "dense_30 (Dense)                 (None, 17)            127517      dense_input_5[0][0]              \n",
      "====================================================================================================\n",
      "Total params: 127517\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "model = model.fit(X_train_new, y_train, batch_size = 256,\n",
    "          nb_epoch = 100, verbose=0, validation_data=(X_test_new,y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4512/4685 [===========================>..] - ETA: 0s"
     ]
    }
   ],
   "source": [
    "predicted = model.model.predict_classes(X_test_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Results\n",
      "Confusion Matrix: \n",
      "[[   0    0    0    0    0    0    0    0    0  129    0    0    0    0\n",
      "     0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0   96    0    0    0    0\n",
      "     0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0   73    0    0    0    0\n",
      "     0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0   35    0    0    0    0\n",
      "     0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0  816    0    0    0    0\n",
      "     0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0   86    0    0    0    0\n",
      "     0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0   16    0    0    0    0\n",
      "     0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0  115    0    0    0    0\n",
      "     0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0   24    0    0    0    0\n",
      "     0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0 2523    0    0    0    0\n",
      "     0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0  195    0    0    0    0\n",
      "     0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0   33    0    0    0    0\n",
      "     0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    2    0    0    0    0\n",
      "     0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0  143    0    0    0    0\n",
      "     0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0   19    0    0    0    0\n",
      "     0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0  206    0    0    0    0\n",
      "     0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0  174    0    0    0    0\n",
      "     0    0    0]]\n",
      "\n",
      " Classifcation Report\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.00      0.00      0.00       129\n",
      "          1       0.00      0.00      0.00        96\n",
      "          2       0.00      0.00      0.00        73\n",
      "          3       0.00      0.00      0.00        35\n",
      "          4       0.00      0.00      0.00       816\n",
      "          5       0.00      0.00      0.00        86\n",
      "          6       0.00      0.00      0.00        16\n",
      "          7       0.00      0.00      0.00       115\n",
      "          8       0.00      0.00      0.00        24\n",
      "          9       0.54      1.00      0.70      2523\n",
      "         10       0.00      0.00      0.00       195\n",
      "         11       0.00      0.00      0.00        33\n",
      "         12       0.00      0.00      0.00         2\n",
      "         13       0.00      0.00      0.00       143\n",
      "         14       0.00      0.00      0.00        19\n",
      "         15       0.00      0.00      0.00       206\n",
      "         16       0.00      0.00      0.00       174\n",
      "\n",
      "avg / total       0.29      0.54      0.38      4685\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# put the y_test back into a format of non-one-hot encoded for comparison\n",
    "y_test_orig = pd.DataFrame(y_test).idxmax(1)\n",
    "print \"Logistic Regression Results\\nConfusion Matrix: \"\n",
    "print confusion_matrix(y_test_orig,predicted)\n",
    "print \"\\n Classifcation Report\"\n",
    "print classification_report(y_test_orig,predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the same results as with the previous logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We will try sklearn's logistic regression.\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# regular\n",
    "y_train_orig = pd.DataFrame(y_train).idxmax(1)\n",
    "y_test_orig = pd.DataFrame(y_test).idxmax(1)\n",
    "\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train_new, y_train_orig)\n",
    "predicted = lr.predict(X_test_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Results\n",
      "Confusion Matrix: \n",
      "[[   3    1    1    0   20    0    0    4    0   92    3    0    0    0\n",
      "     0    3    2]\n",
      " [   5    2    2    0   15    2    0    2    0   58    4    0    0    1\n",
      "     0    4    1]\n",
      " [   3    2    3    0   14    0    0    0    0   48    2    0    0    0\n",
      "     0    0    1]\n",
      " [   0    0    0    0   11    0    0    1    0   19    1    0    0    2\n",
      "     0    1    0]\n",
      " [  26    4    2    1  172    6    0    9    0  506   30    1    0   11\n",
      "     1   29   18]\n",
      " [   7    2    0    0   10    3    0    1    0   56    1    1    0    2\n",
      "     0    2    1]\n",
      " [   1    0    0    0    3    1    0    0    0    9    0    0    0    1\n",
      "     0    1    0]\n",
      " [   3    1    1    0   24    1    0    2    0   70    5    1    0    0\n",
      "     0    5    2]\n",
      " [   0    0    0    0    5    0    0    1    1   17    0    0    0    0\n",
      "     0    0    0]\n",
      " [  26    4    3    0  250    7    0    6    1 2117   43    0    0   17\n",
      "     0   35   14]\n",
      " [   2    0    0    0   28    1    0    2    0  128   21    0    0    4\n",
      "     0    7    2]\n",
      " [   1    1    0    0    5    0    0    0    0   25    0    0    0    0\n",
      "     0    1    0]\n",
      " [   0    0    0    0    0    0    0    0    0    1    0    0    0    0\n",
      "     0    1    0]\n",
      " [   2    2    1    0   18    2    0    1    0   99    3    0    0    8\n",
      "     0    6    1]\n",
      " [   0    0    0    0    1    0    0    2    0    9    5    0    0    0\n",
      "     1    1    0]\n",
      " [   6    0    2    0   28    6    0    2    0  122   21    0    0    8\n",
      "     0    9    2]\n",
      " [   5    2    0    0   32    6    0    1    0  112    7    0    0    1\n",
      "     0    1    7]]\n",
      "\n",
      " Classifcation Report\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.03      0.02      0.03       129\n",
      "          1       0.10      0.02      0.03        96\n",
      "          2       0.20      0.04      0.07        73\n",
      "          3       0.00      0.00      0.00        35\n",
      "          4       0.27      0.21      0.24       816\n",
      "          5       0.09      0.03      0.05        86\n",
      "          6       0.00      0.00      0.00        16\n",
      "          7       0.06      0.02      0.03       115\n",
      "          8       0.50      0.04      0.08        24\n",
      "          9       0.61      0.84      0.70      2523\n",
      "         10       0.14      0.11      0.12       195\n",
      "         11       0.00      0.00      0.00        33\n",
      "         12       0.00      0.00      0.00         2\n",
      "         13       0.15      0.06      0.08       143\n",
      "         14       0.50      0.05      0.10        19\n",
      "         15       0.08      0.04      0.06       206\n",
      "         16       0.14      0.04      0.06       174\n",
      "\n",
      "avg / total       0.41      0.50      0.44      4685\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print \"Logistic Regression Results\\nConfusion Matrix: \"\n",
    "print confusion_matrix(y_test_orig,predicted)\n",
    "print \"\\n Classifcation Report\"\n",
    "print classification_report(y_test_orig,predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With SciKitLearn's logistic regression algorithm, we get much better (but not good) precision than with the \n",
    "previous methods (excluding convolutional neural networks), but worse recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,\n",
       "            oob_score=False, random_state=47, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Finally, we'll try the random forest, which is not known for being great with image data.\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "rfc = RandomForestClassifier(random_state=47, n_estimators=100)\n",
    "rfc.fit(X_train_new, y_train_orig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Results\n",
      "Confusion Matrix: \n",
      "[[   2    0    0    0   17    0    0    1    0  109    0    0    0    0\n",
      "     0    0    0]\n",
      " [   0    2    1    0   25    0    0    0    0   67    0    0    0    0\n",
      "     0    0    1]\n",
      " [   0    0    2    0   14    0    0    0    0   57    0    0    0    0\n",
      "     0    0    0]\n",
      " [   0    0    0    0    9    0    0    0    0   26    0    0    0    0\n",
      "     0    0    0]\n",
      " [   1    0    0    1  218    0    0    1    0  595    0    0    0    0\n",
      "     0    0    0]\n",
      " [   0    0    0    0   14    2    0    0    0   70    0    0    0    0\n",
      "     0    0    0]\n",
      " [   0    0    0    0    4    0    0    0    0   12    0    0    0    0\n",
      "     0    0    0]\n",
      " [   0    0    0    0   24    0    0    0    0   90    0    1    0    0\n",
      "     0    0    0]\n",
      " [   0    0    0    0    4    0    0    0    1   19    0    0    0    0\n",
      "     0    0    0]\n",
      " [   1    0    0    0   51    0    0    0    0 2471    0    0    0    0\n",
      "     0    0    0]\n",
      " [   0    0    0    0   23    0    0    0    0  163    9    0    0    0\n",
      "     0    0    0]\n",
      " [   0    1    0    0    5    0    0    0    0   27    0    0    0    0\n",
      "     0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    2    0    0    0    0\n",
      "     0    0    0]\n",
      " [   0    0    0    0   28    0    0    0    0  113    0    0    0    2\n",
      "     0    0    0]\n",
      " [   0    0    0    0    2    0    0    0    0   16    0    0    0    0\n",
      "     1    0    0]\n",
      " [   0    0    0    0   63    0    0    0    0  141    0    0    0    0\n",
      "     0    2    0]\n",
      " [   0    0    0    0   53    1    0    0    0  118    0    0    0    0\n",
      "     0    0    2]]\n",
      "\n",
      " Classifcation Report\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.50      0.02      0.03       129\n",
      "          1       0.67      0.02      0.04        96\n",
      "          2       0.67      0.03      0.05        73\n",
      "          3       0.00      0.00      0.00        35\n",
      "          4       0.39      0.27      0.32       816\n",
      "          5       0.67      0.02      0.04        86\n",
      "          6       0.00      0.00      0.00        16\n",
      "          7       0.00      0.00      0.00       115\n",
      "          8       1.00      0.04      0.08        24\n",
      "          9       0.60      0.98      0.75      2523\n",
      "         10       1.00      0.05      0.09       195\n",
      "         11       0.00      0.00      0.00        33\n",
      "         12       0.00      0.00      0.00         2\n",
      "         13       1.00      0.01      0.03       143\n",
      "         14       1.00      0.05      0.10        19\n",
      "         15       1.00      0.01      0.02       206\n",
      "         16       0.67      0.01      0.02       174\n",
      "\n",
      "avg / total       0.59      0.58      0.47      4685\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predicted = rfc.predict(X_test_new)\n",
    "\n",
    "print \"Decision Tree Results\\nConfusion Matrix: \"\n",
    "print confusion_matrix(y_test_orig,predicted)\n",
    "print \"\\n Classifcation Report\"\n",
    "print classification_report(y_test_orig,predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The random forest is actually working much better than the neural networks and logistic regression models, and better precision than even the convolutional neural network.  The convolutional neural network still has the best recall, however.  It's still not a great model, but it has precision and recall better then 50%, however, most test records are being predicted as 9 as in the other models.  Apparently, our data is unbalanced among the classes - there are tons of category 'plain' compared to others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "g=imgdf.groupby(['category']).count()['_unit_id']\n",
    "\n",
    "objects = g.index\n",
    "y_pos = np.arange(len(objects))\n",
    "performance = g\n",
    "\n",
    "plt.bar(y_pos, performance, align='center', alpha=0.5)\n",
    "#plt.xticks(y_pos, objects)\n",
    "plt.xticks(y_pos, objects, rotation='vertical')\n",
    "plt.ylabel('Class Counts')\n",
    "plt.title('Dress Classes in Data')\n",
    " \n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
